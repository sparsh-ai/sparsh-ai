<!DOCTYPE html>
<html lang="en">

<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5F9Y9HC95G"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5F9Y9HC95G');
</script>
<script data-ad-client="ca-pub-8325812071117232" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <!-- Chrome, Firefox OS and Opera Status Bar Color -->
  <meta name="theme-color" content="#FFFFFF">
  <meta property="og:title" content="Extras">
    <meta property="og:type" content="blog">
  <title>Extras</title>
<link rel="icon" type="image/png" href="/favicon.png"/>
  <!-- Favicon -->
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
  <link rel="stylesheet" type="text/css"
    href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
  <link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
  <link rel="stylesheet" type="text/css" href="css/notablog.css">
  <link rel="stylesheet" type="text/css" href="css/theme.css">
  <style>
    :root {
      font-size: 18px;
    }

    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
    <a href="index">
      <div class="Navbar__Btn"> <span>Home</span></div>
    </a>
                                                                                                                            <span class="Navbar__Delim">&centerdot;</span>
    <a href="extras">
      <div class="Navbar__Btn"> <span>Extras</span></div>
    </a>
                <span class="Navbar__Delim">&centerdot;</span>
    <a href="about">
      <div class="Navbar__Btn"> <span>About</span></div>
    </a>
                <span class="Navbar__Delim">&centerdot;</span>
    <a href="news">
      <div class="Navbar__Btn"> <span>News</span></div>
    </a>
          </nav>
  <header class="Header">
          <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
        <h1 class="Header__Title">Extras</h1>
          </header>
      <article id="https://www.notion.so/2b919479b28e487184520fc06346f4f3" class="PageRoot"><h2 id="https://www.notion.so/f2603bad4d77422aaed5fc6e1e053dac" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/f2603bad4d77422aaed5fc6e1e053dac"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">üìí Client Projects</span></span></h2><div id="https://www.notion.so/38adfb8a0603455eb11318911ac013bb" class="ColorfulBlock ColorfulBlock--BgBlue Callout"><div class="Callout__Icon"><div class="Icon">üîí</div></div><p class="Callout__Content"><span class="SemanticStringArray"><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Note: Sharing limited information only, following the confidentiality policies.</em></span></span></p></div><details id="https://www.notion.so/80e427ab6f524e9cb79b88ce500cabce" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">Claim Severity for Reinsurance</span></span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/a7a728b7e5f14ee798b397d3f7c95684" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">In a typical insurance claims scenario, the insurance company gets a large number of small amounts of claims and few high amounts of claims. In statistical terms, we call it a body-trail curve, where the area under the curve represents the probability of a particular (x-axis) claim amount. So tail signifies a low probability but very high amount (severity) claims. Since there are only a few historical events in the history of this level of severity, how can we properly fit a model on this kind of data in order to correctly estimate the probability (area under the fitted curve) of the severity-level?</span></span></p></div><div id="https://www.notion.so/1a26289886e544f095e5c7a0b28c480a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">I experimented with different kinds of data transformation scales (log-scale, double-log scale, log-sigma scale, etc.) and multiple models (negative exponential, gamma, beta, Gumbel, etc.) and finally settled with a spliced curve model: multiple erlangs on body and Gumbel (from EVT class) on the tail. The AIC and BIC criteria were used for evaluation and model evaluation.</span></span></p></div></div></details><details id="https://www.notion.so/6b883c74d3b7458dba47330f724ff0b3" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">Insurance Chatbot &amp; Voicebot</span></span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/c62d887951c046698b6ea247db7e5a22" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Problem statement: How to automate the insurance contact center services in order to reduce operational costs?, How to reduce the operational cost of contact center agents?, How to manage the SLAs of customer service in case of labor shortage?, How to reduce the resolution time of insurance customer‚Äôs queries and complaints?</span></span></p></div><div id="https://www.notion.so/fb9036e561be470b9b87e2ddf630930a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Scope: Insurance sector, automate 4 services - claim status check, claim filing, policy quotation, and policy education. The language would be English (US &amp; UK). RASA framework (python) was selected for the implementation of this chatbot.</span></span></p></div><div id="https://www.notion.so/7484d0c0d4a54131a61cc46ef95647c1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">A voice service was also ingegrated on the claim status check service. Various options were explored and simple proof-of-concepts were developed for each of them. A comparison chart was also created as shown below: </span></span></p></div><div id="https://www.notion.so/ba69438ee74142b783e4fda9c7f5b4d0" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6410a46f-9b17-405e-be09-abb96f63d516%2FUntitled.png?width=1202&amp;table=block&amp;id=ba69438e-e741-42b7-83e4-fda9c7f5b4d0"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6410a46f-9b17-405e-be09-abb96f63d516%2FUntitled.png?width=1202&amp;table=block&amp;id=ba69438e-e741-42b7-83e4-fda9c7f5b4d0" style="width:100%"/></a><figcaption><span class="SemanticStringArray"><span class="SemanticString">Voicebot platform comparison</span></span></figcaption></figure></div></div></details><details id="https://www.notion.so/7a6ab64ee3aa47d0a637174981d87f37" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">Pre-auth Approval Insurance Clinical Decision Making</span></span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/1754235aa2c84941b3d61ec1b0b682f7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Health insurance can be complicated‚Äîespecially when it comes to prior authorization (also referred to as pre-approval, pre-authorization, and pre-certification). The manual labor involved in obtaining prior authorizations (PAs) is a well-recognized burden among providers. Up to 46% of PA requests are still submitted by fax, and 60% require a telephone call, according to America‚Äôs Health Insurance Plans (AHIP). A 2018 survey by the American Medical Association (AMA) found that doctors and their staff spend an average of 2 days a week completing PAs. In addition to eating up time that physicians could spend with patients, PAs also contribute to burnout.</span></span></p></div><div id="https://www.notion.so/1c1494463b104b218a91b8119967189a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The objective was to identify the patterns from data to create clinical decision making in Pre-Auth and improve the accuracy in a clinical decision based on historical data analysis. </span></span></p></div><div id="https://www.notion.so/0390b50f577e4947b24e3b8fff869b80" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Two use cases were identified. Use Case 1 - </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Supervised Learning Model - to aid clinicians in UM decision making. Tasks - </em></span><span class="SemanticString">Ingest Pre-authorization data from Mongo DB into the analytical environment, Exploratory Data Analysis and Feature Engineering, Train supervised analytical models, model validation and model selection, Create a web service to be plugged into the case processing flow to call the model, and Display the recommendation from the model on UI on the authorization review screen. Use Case 2 - </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Unsupervised Learning Model - to generate insights from the pre-authorization data. Tasks - </em></span><span class="SemanticString">Ingest Pre-authorization data from Mongo DB into the analytical environment, Cluster analysis, univariate and multivariate analysis, and Generate insights and display insights on the dashboard.</span></span></p></div><div id="https://www.notion.so/cf8a1245762c4e769410a6cce80f2510" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Final Deliverables - Model re-training (batch mode), validation and deployment code (python scripts) with Unix command line support, Documentation - PPT, Recorded video, Technical document, Flask API backend system, HTML/PHP Web App frontend UI integration, and Plotly Dash Supervised/Unsupervised learning and insights generation dashboard.</span></span></p></div></div></details><details id="https://www.notion.so/c8e489f6e0974f7d84b0ee866fa04b93" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">IT Support Ticket Management</span></span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/ce6f5388dc624b65a9df02881af9e639" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">In Helpdesk, almost 30‚Äì40% of incident tickets are not routed to the right team and the tickets keep roaming around and around and by the time it reaches the right team, the issue might have widespread and reached the top management inviting a lot of trouble. Let‚Äôs say that users are having some trouble with printers. User calls help desk, he creates a ticket with IT Support, and they realize that they need to update a configuration in the user‚Äôs system and then they resolve the ticket. What if 10 other users report the same issue. </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">How to better manage these common issues using analytics tools and techniques?</strong></span></span></p></div><div id="https://www.notion.so/d5f35641d6a14ec1b68d73c9e7d6e1ce" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Identified solutions: Key Phrase Analysis - Identify key-phrases using RAKE algorithm. Analyze frequency and plot word cloud visual for most common phrases. Topic Modeling - Divide the phrases into topics using LDA/LSA algorithm. Ticket Classification - Classify ticket into categories using ML classification algorithm. Trend, Seasonality, and Outlier Analysis - Weekly/ Monthly/ Quarterly/ Yearly trend in ticket volume using Moving Average and other similar algorithms. Identify outliers using IQR and other similar methods. PowerBI Dashboard - To visually represent the KPIs. Standard vs. Non-standard Template Responses - If the issue is getting repeated in multiple tickets (above a threshold), then it is labeled as standard-template.</span></span></p></div></div></details><details id="https://www.notion.so/6f86a772381242348dd2585e25ff1f88" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">ServiceNow Advanced Analytics</span></span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/5e38260e2461474a8330718df1cdac72" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">An insurance provider started using ServiceNow a few years ago, to catalog and process customer service requests. The incident descriptions are being analyzed manually to derive primary issues raised by customers, communicated with stakeholders. As the client grew rapidly, it was flooded with huge volumes of customer service requests. Eventually, the requests branched out into multiple service lines. The client has a wealth of text-based data sources, especially from ServiceNow, that require manual review in order to </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">determine themes and emerging trends</strong></span><span class="SemanticString">. The client wanted to </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">remove the dependency on human-based reviews</strong></span><span class="SemanticString"> and extract more value from the wealth of information. </span></span></p></div><div id="https://www.notion.so/34fa4f0afd45429e819f3af1a5ecf107" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Initial set of tasks: Explore data science approaches for extracting value from our text based information, Grouping together similar phrases (e.g. incident summary, description, resolution notes), thereby grouping incident records of similar nature ‚Äì using the same grouping mechanism, this could lead to the best matching set of existing problem records, Excluding common words/terms ‚Äì a lot of tickets are raised via templates and have very similar wording ‚Äì this provides very little value (consider Term Frequency / Inverse Document Frequency approach that reduces the search value of commonly used words), Consider stemming algorithms and others to align similar words (e.g. broken, not working, unavailable, not available), and Analytical and visualisation techniques to improve detection of emerging trends.</span></span></p></div><div id="https://www.notion.so/b34bba569081461ea9d2a7de6ae4026c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Identified solutions: Loading data from MS SQL Server onto to python, Restructuring data to enrich it and make it available in more usable form - Consolidating various data tables into a single data layer, Heatmap of month-wise incident volume for top service lines,  Custom stopwords removal, Lemmatization, Lower-casing, Tokenization, RAKE to capture emerging themes from textual data - Topic modeling to understand relationship between themes captured, Design various dashboard screens to demonstrate outlier analysis, word cloud comparison, the relationship between emerging themes, and Create a task scheduler to execute on a weekly, monthly, annual basis.</span></span></p></div></div></details><details id="https://www.notion.so/a5687f45f9d843dd870ceb1fd5910d55" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">Cyber Risk Quantification</span></span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/e4841dce2f664a909e800ee10334a734" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Economic and commercial operations have become increasingly reliant on digital technologies which face a constant threat of disruption due to human error or malicious attacks. The potential for serious economic and commercial repercussions, illustrated most recently in the millions of compromised records at Yahoo and Equifax, the disruption of major websites by a denial-of-service attack on Dyn and the hundreds of thousands of computers compromised by the WannaCry and NotPetya ransomware attacks, has meant increasing investment in safeguarding the confidentiality, integrity and availability of information and information systems.</span></span></p></div><div id="https://www.notion.so/30983fbd7aca46a0ac34cb55a810028b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The usual approach of managing information security risk is similar to other business risks, i.e. first eliminate, then mitigate, absorb, and then if possible, transfer. Since eliminating security risks in today‚Äôs environment is not possible, managers deploy protection technologies like firewall, antivirus, encryption, and instate appropriate security policies like passwords, access control, port blocking, etc. to mitigate the probability of a break-in or failure. If the residual risk is manageable it is absorbed, otherwise, transferred by either outsourcing security or buying insurance.</span></span></p></div><div id="https://www.notion.so/e6095f2f2dd243a1bcf8463821dea72f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">While not a substitute for investing in cybersecurity and risk management‚Äîas having good cybersecurity and avoiding disruption is a more preferable outcome‚Äî insurance coverage for cyber risk can make an important contribution to the management of cyber risk by promoting awareness about exposure to cyber losses, sharing expertise on risk management, encouraging investment in risk reduction and facilitating the response to cyber incidents. </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">How to quantify the cyber risk of a company for profitable ratemaking?</strong></span></span></p></div><div id="https://www.notion.so/03034764fe674de58a3adb2a65dd4a89" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Identified solutions: Baseline cyber risk quantification using FAIR framework including Beta PERT and Monte Carlo calibration, Data-driven IT security posture assessment using LSTM and CNN deep learning models, Macro environment risk adjustment by advanced frequency, severity and copula dependency modeling on historical data, and Accumulation risk adjustment using scenario-based portfolio risk assessment methodologies.</span></span></p></div></div></details><details id="https://www.notion.so/047e5f0a0cb74d6dbef83538e41a9005" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">Contact Center Analytics</span></span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/1f367b43a5064997bddcea5d533af48f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Objectives: Increase Customer Satisfaction, Reduce Query Resolution Time, Reduce Operational Cost, and Increase Gross Profit.</span></span></p></div><div id="https://www.notion.so/448f9c7f51c343ac9acfc741d00e7d8b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Identified solutions - Increase customer satisfaction by 1) reduce the waiting time, and 2) Connect with the right agent. This can be achieved by intelligent customer routing analytics. Reduce query resolution time by 1) Automated query resolution, 2) Connect with the right agent, and 3) Agent real-time assistance. This can be achieved by the chatbot and intelligent routing analytics. Reduce operational cost by 1) hiring the optimal number of agents, and 2) customer segmentation and prioritization. This can be achieved by contact volume forecasting and agent staffing analytics. Increase gross profit by 1) Identifying churning customers and retention, and 2) Cross-sell and upsell during inbound contact. This can be achieved by churn modeling and cross-sell/up-sell modeling. </span></span></p></div></div></details><details id="https://www.notion.so/bc14dbbef5564a32b2d9c2ef18df20ee" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">Multi Touch Attribution</span></span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/76d20297cbb24558b613137ae8c96f28" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">We are living in the digital economy and a customer often exposed to promotional ads on different digital channels like Facebook and google search. If a customer purchased a product after multiple exposures on different channels, then how can we estimate each channel&#x27;s individual contribution towards that conversion, so that we can assign the marketing budget accordingly?  </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">How to quantify the impact of different media channels on sales volume?</strong></span></span></p></div><div id="https://www.notion.so/cced30cc826c4cf88d6e26798e1cafb2" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Multi-touch attribution is a set of methods and modeling techniques that tries to estimate the digital channel&#x27;s individual contribution leveraging historical data of customer touchpoints and machine learning methodologies. I started with searching for relevant research papers in this domain, performed a literature review, and created a framework. I experimented with a lot of models and finally settled with three: Markov chains, Survival analysis, and RNN with attention. Conversion prediction accuracy was used as a proxy for evaluation and model selection but the A/B test should also be done before deploying this model in production.</span></span></p></div></div></details><details id="https://www.notion.so/12cf1570e502456e8fd497acc675f67d" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">Claim Process Automation using Image Analytics</span></span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/f4520955b1ae4ec0a7356591f36cafc0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Today, in the car insurance industry, a lot of money is wasted due to claims leakage. Claims leakage / Underwriting leakage is defined as the difference between the actual claim payment made and the amount that should have been paid if all industry-leading practices were applied. Visual inspection and validation have been used to reduce such effects. However, they introduce delays in claim processing and lead to manual intervention. Fast and efficient claims processing is paramount to success for insurance companies. </span></span></p></div><div id="https://www.notion.so/8726d264be20443ea9d4321550cd8af5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Automatic assessment of the damages through image analysis is much faster and more accurate and it will become even better as they collect more &amp; more data for each use case. With deep learning, we can automatically detect scratches, dents, rust, breakages. We can also detect which part of the vehicle is damaged and with what severity. The vehicle can be automatically inspected using images or video feeds by creating a 360¬∞ overview. After the inspection, the report can be generated with a list of damages and estimate cost repair.</span></span></p></div><div id="https://www.notion.so/bbdf044c971741a883a07f7116166f70" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Identified solutions: Customer uploads an image of their damaged vehicle on the web portal/mobile app. Image Forgery Verification - Because there is a possibility of fraud by uploading fake/forged images, this model will verify if image is real or forged by some software. Vehicle Identity Verification - This model will verify if the damaged vehicle in image is the same vehicle for which the claim has been filed (damaged image of similar looking vehicle might be used to fraud the system). Data Sufficiency Verification - We will verify if images are of correct quality (not blurred, taken from the right angle) and also sufficient (enough number of images as reuqired by the model for correctly assess the damage). Distance Adjustment - We will adjust the distance of perspective to normalize the damage area pixels in the image. Depth Analysis - We will also measure the depth of the damage. Image Segmentation - This model will classify the pixels of the image into damage categories. We will use this information to assess the damage.</span></span></p></div></div></details><div id="https://www.notion.so/4ead9e800fff4109b47b7e9c0d2a84e2" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1211d0640f484fd1bb117dd0d344d176" class="Divider"></div><h2 id="https://www.notion.so/a3ea68a24efd4a50ad6bc32d5fbbc9ec" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/a3ea68a24efd4a50ad6bc32d5fbbc9ec"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">üìî Rough Notes</span></span></h2><details id="https://www.notion.so/057e19d001d0407cb6be0fb77dfed4bb" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">Image Translation</span></span></summary><div class="Toggle__Content"><details id="https://www.notion.so/1cc3e03bd7a542ec8ea08d77263d5b07" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">Image to Image Translation</span></span></summary><div class="Toggle__Content"><h2 id="https://www.notion.so/18fd7fea00cb412b9acfe5eb07771ba0" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/18fd7fea00cb412b9acfe5eb07771ba0"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">INTRODUCTION</span></span></h2><div id="https://www.notion.so/f0572f85fdc44e8abc9f45b06cd70ca7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Many image processing, computer graphics, and computer vision problems can be treated as image-to-image translation tasks. Such translation entails learning to map one visual representation of a given input to another representation.</span></span></p></div><div id="https://www.notion.so/f97b4a2041c4459b92a69058a2b4e6c5" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F03a278b3-dd29-47da-8aa2-55759d642263%2FUntitled.png?width=1303&amp;table=block&amp;id=f97b4a20-41c4-459b-92a6-9058a2b4e6c5"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F03a278b3-dd29-47da-8aa2-55759d642263%2FUntitled.png?width=1303&amp;table=block&amp;id=f97b4a20-41c4-459b-92a6-9058a2b4e6c5" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/e744184e0d694a8aad2aaf64bd38dbd2" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F18f9a736-8e5f-4767-a7a0-81f43189b085%2FUntitled.png?width=1154&amp;table=block&amp;id=e744184e-0d69-4a8a-ad2a-af64bd38dbd2"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F18f9a736-8e5f-4767-a7a0-81f43189b085%2FUntitled.png?width=1154&amp;table=block&amp;id=e744184e-0d69-4a8a-ad2a-af64bd38dbd2" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/fe7e1f64a2384bfdb7e5795d0c44d452" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa2eab1ed-98f4-4988-b3ab-0b5f37b79e6e%2FUntitled.png?width=1004&amp;table=block&amp;id=fe7e1f64-a238-4bfd-b7e5-795d0c44d452"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa2eab1ed-98f4-4988-b3ab-0b5f37b79e6e%2FUntitled.png?width=1004&amp;table=block&amp;id=fe7e1f64-a238-4bfd-b7e5-795d0c44d452" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/83d036da53564ca78a22b99610076b5f" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff12d6c20-2040-4856-b349-277685e37bdf%2FUntitled.png?width=471&amp;table=block&amp;id=83d036da-5356-4ca7-8a22-b99610076b5f"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff12d6c20-2040-4856-b349-277685e37bdf%2FUntitled.png?width=471&amp;table=block&amp;id=83d036da-5356-4ca7-8a22-b99610076b5f" style="width:471px"/></a><figcaption><span class="SemanticStringArray"><span class="SemanticString">Style control</span></span></figcaption></figure></div><div id="https://www.notion.so/02865a379481476095666974c3467129" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc7c90101-620e-45a3-9af9-6339758d8f98%2FUntitled.png?width=2125&amp;table=block&amp;id=02865a37-9481-4760-9566-6974c3467129"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc7c90101-620e-45a3-9af9-6339758d8f98%2FUntitled.png?width=2125&amp;table=block&amp;id=02865a37-9481-4760-9566-6974c3467129" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/a1cd1ddb5083478aadf87c32f7699656" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h2 id="https://www.notion.so/ab682334b45746148c0ccf2edaa9e31d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/ab682334b45746148c0ccf2edaa9e31d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">METHODOLOGIES</span></span></h2><div id="https://www.notion.so/e2dedd5c27f84e81a7bfd4626bef8779" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc9528f64-7576-4e92-b748-33c852b04a47%2FUntitled.png?width=792&amp;table=block&amp;id=e2dedd5c-27f8-4e81-a7bf-d4626bef8779"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc9528f64-7576-4e92-b748-33c852b04a47%2FUntitled.png?width=792&amp;table=block&amp;id=e2dedd5c-27f8-4e81-a7bf-d4626bef8779" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h3 id="https://www.notion.so/422dd008594040a9bef83476d2d36f65" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/422dd008594040a9bef83476d2d36f65"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Paired Image to Image Translation</span></span></h3><div id="https://www.notion.so/108dbf63a6ff4a11ae54f534e8b48f56" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F233160a7-7114-43b6-95d5-c7d09f9fd570%2FUntitled.png?width=1226&amp;table=block&amp;id=108dbf63-a6ff-4a11-ae54-f534e8b48f56"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F233160a7-7114-43b6-95d5-c7d09f9fd570%2FUntitled.png?width=1226&amp;table=block&amp;id=108dbf63-a6ff-4a11-ae54-f534e8b48f56" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/42d82c7256cf4c87acbc58bf1d7149d1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/e2396b9477874e878c6ebbca797de034" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/e2396b9477874e878c6ebbca797de034"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Designing the Loss Function</span></span></h3><details id="https://www.notion.so/8eb9a82237164d5ba652329302b80070" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">CVPR 2016 PPT Selected slides - Instead of hand-designing loss, GAN Discriminator is the learned loss function</span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/75d399ed6d994101a032f214a16922c8" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F2862d005-e8ec-4fdf-a9b4-fe59aa1adcb4%2FUntitled.png?width=851&amp;table=block&amp;id=75d399ed-6d99-4101-a032-f214a16922c8"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F2862d005-e8ec-4fdf-a9b4-fe59aa1adcb4%2FUntitled.png?width=851&amp;table=block&amp;id=75d399ed-6d99-4101-a032-f214a16922c8" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/559f15c1a4f04a6b882e351b503a4a3d" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe5f5a72c-6623-4c38-bd7d-6f81edfb630a%2FUntitled.png?width=463&amp;table=block&amp;id=559f15c1-a4f0-4a6b-882e-351b503a4a3d"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe5f5a72c-6623-4c38-bd7d-6f81edfb630a%2FUntitled.png?width=463&amp;table=block&amp;id=559f15c1-a4f0-4a6b-882e-351b503a4a3d" style="width:463px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/3f434993b3464a9d852485ba3d5b5846" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F0fe6b948-ef18-4c1b-a3ea-b2aa202f4290%2FUntitled.png?width=776&amp;table=block&amp;id=3f434993-b346-4a9d-8524-85ba3d5b5846"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F0fe6b948-ef18-4c1b-a3ea-b2aa202f4290%2FUntitled.png?width=776&amp;table=block&amp;id=3f434993-b346-4a9d-8524-85ba3d5b5846" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/1f8362e482404564a7a3b7996c038c5b" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Feaf48203-f2ad-4a15-bfb8-0ae79539cafc%2FUntitled.png?width=610&amp;table=block&amp;id=1f8362e4-8240-4564-a7a3-b7996c038c5b"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Feaf48203-f2ad-4a15-bfb8-0ae79539cafc%2FUntitled.png?width=610&amp;table=block&amp;id=1f8362e4-8240-4564-a7a3-b7996c038c5b" style="width:610px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/a9d6347403c3446cb9848f2e099b7bfa" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fabe8f9d1-9f69-4bde-98b8-730861e1ebed%2FUntitled.png?width=782&amp;table=block&amp;id=a9d63474-03c3-446c-b984-8f2e099b7bfa"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fabe8f9d1-9f69-4bde-98b8-730861e1ebed%2FUntitled.png?width=782&amp;table=block&amp;id=a9d63474-03c3-446c-b984-8f2e099b7bfa" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div></div></details><h3 id="https://www.notion.so/56aa13784a2f4418b656bf884b3bd92d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/56aa13784a2f4418b656bf884b3bd92d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Patch Discriminator</span></span></h3><div id="https://www.notion.so/5edefe8172bc4d06abf93eaa8262e497" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff9585909-98fc-4263-bfde-7906266a5550%2FUntitled.png?width=796&amp;table=block&amp;id=5edefe81-72bc-4d06-abf9-3eaa8262e497"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff9585909-98fc-4263-bfde-7906266a5550%2FUntitled.png?width=796&amp;table=block&amp;id=5edefe81-72bc-4d06-abf9-3eaa8262e497" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/73300d345fed4f86b3fbb84506558d7d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/38649d2b968e42acb30249c0c3671eae" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/38649d2b968e42acb30249c0c3671eae"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Pix2Pix</span></span></h3><div id="https://www.notion.so/88d099df3de24a7180ca4d5f718cfbb0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">‚≠ê </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://machinelearningmastery.com/how-to-develop-a-pix2pix-gan-for-image-to-image-translation/">Blog guide</a></span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/8979023c9b7249a6b35bb8db40bab7a3" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Pix2Pix Browser </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://affinelayer.com/pixsrv/">demo</a></span></span></li></ul><div id="https://www.notion.so/8e585ac8230f4abcb3fb723be54d0527" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h2 id="https://www.notion.so/94b7fca91b8e486e860c620e2cb8855c" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/94b7fca91b8e486e860c620e2cb8855c"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">EXPERIMENTATION</span></span></h2><h3 id="https://www.notion.so/f197e703e5f94142a7a4a0b8695abf6c" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/f197e703e5f94142a7a4a0b8695abf6c"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Inference</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/8f2ff678849741ccb9f430130d94cbdd" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">CycleGAN Pytorch </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb">colab</a></span></span></li><li id="https://www.notion.so/de26025e74f049ff9b8fc2caf415bc94" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Pix2Pix Pytorch </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb">colab</a></span></span></li></ul><h3 id="https://www.notion.so/021ae186ba93457393bd717b8d4b7454" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/021ae186ba93457393bd717b8d4b7454"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Training</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/3eb61e2dd2ae49bea77ee416e4bdff69" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Train CycleGAN and Pix2Pix in Pytorch </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">here</a></span></span></li><li id="https://www.notion.so/7a6a9c5e18f944928d390e23e665769e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">CycleGAN from scratch </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://theailearner.com/tag/image-to-image-translation/">blog</a></span></span></li><li id="https://www.notion.so/86ccc98d54cf42d89e4f1a9054d68e5a" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">CycleGAN from scratch </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://machinelearningmastery.com/cyclegan-tutorial-with-keras/">blog</a></span></span></li><li id="https://www.notion.so/e4805b29b9d24e308354f2bc20e85192" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">CycleGAN from scratch </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://keras.io/examples/generative/cyclegan/">Keras Official</a></span></span></li><li id="https://www.notion.so/541cc0df465a4da79083428ee15a550f" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Pix2Pix from scratch code </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://machinelearningmastery.com/how-to-develop-a-pix2pix-gan-for-image-to-image-translation/">blog</a></span></span></li><li id="https://www.notion.so/93f6cd50bce4491e9415d31ec26b747d" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">StyleGAN2 custom training </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/dvschultz/stylegan2-ada">git</a></span></span></li><li id="https://www.notion.so/fee947e4a74442bf89cd233780feed87" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Custom Pix2PixHD </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://youtu.be/fXgodCC2O7o">youtube</a></span></span></li><li id="https://www.notion.so/94cb65bc61f34269b0988429e66de077" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Packt Notebooks </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/sparsh-ai/Deep-Learning-with-TensorFlow-2-and-Keras/tree/master/Chapter%206">git</a></span></span></li></ul></div></details><details id="https://www.notion.so/db0532d9ccf047bd8366b0352710696e" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">Text to Image Translation</span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/99219b9652fd4eeba6a31aee292feeab" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Generating images from text descriptions is an interesting use case of GANs. This can be helpful in the film industry, as a GAN is capable of generating new data based on some text that you have made up. In the comic industry, it is possible to automatically generate sequences of a story.</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/36ea227448a74c588bc729b6157b282d" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/paarthneekhara/text-to-image">https://github.com/paarthneekhara/text-to-image</a></span></span></li></ul><div id="https://www.notion.so/8e9b83c32cab4b8daab3595e2aeaf3bd" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe246dd2a-ab69-4d10-89a6-5ed6fddb172b%2FUntitled.png?width=1065&amp;table=block&amp;id=8e9b83c3-2cab-4b8d-aab3-595e2aeaf3bd"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe246dd2a-ab69-4d10-89a6-5ed6fddb172b%2FUntitled.png?width=1065&amp;table=block&amp;id=8e9b83c3-2cab-4b8d-aab3-595e2aeaf3bd" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/76bf28337e8d42b0ab2d87b8dec4a053" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">TAC-GAN ‚Äì Text Conditioned Auxiliary Classifier Generative Adversarial Network, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/1703.06412.pdf">[paper]</a></span><span class="SemanticString">, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/dashayushman/TAC-GAN">[github]</a></span></span></li><li id="https://www.notion.so/116d2f6806fd4d1097192b58081a81b2" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/1612.03242.pdf">[paper]</a></span><span class="SemanticString">, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/hanzhanggit/StackGAN">[github]</a></span></span></li><li id="https://www.notion.so/bb8e83003097483e9ccea500e25101c1" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Generative Adversarial Text to Image Synthesis, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/1605.05396.pdf">[paper]</a></span><span class="SemanticString">, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/paarthneekhara/text-to-image">[github]</a></span><span class="SemanticString">, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/reedscot/icml2016">[github]</a></span></span></li><li id="https://www.notion.so/a86fffbe86cf42cb9372038e23db0f4b" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Learning What and Where to Draw, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="http://www.scottreed.info/files/nips2016.pdf">[paper]</a></span><span class="SemanticString">, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/reedscot/nips2016">[github]</a></span></span></li></ul><div id="https://www.notion.so/90e399c211b9430ca6ccdd39d66a5942" class="Bookmark"><a href="https://vision-explorer.allenai.org/text_to_image_generation"><h5 class="Bookmark__Title">Computer Vision Explorer</h5><p class="Bookmark__Desc">The AI2 Computer Vision Explorer offers demos of a variety of popular models - try, compare, and evaluate with your own images!</p><p class="Bookmark__Link">https://vision-explorer.allenai.org/text_to_image_generation</p></a></div><div id="https://www.notion.so/95b4476ad2834ed9b47071b4cca9bdcb" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F178fed76-8303-42d0-81e0-942fad28aae8%2FUntitled.png?width=898&amp;table=block&amp;id=95b4476a-d283-4ed9-b470-71b4cca9bdcb"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F178fed76-8303-42d0-81e0-942fad28aae8%2FUntitled.png?width=898&amp;table=block&amp;id=95b4476a-d283-4ed9-b470-71b4cca9bdcb" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div></div></details></div></details><details id="https://www.notion.so/209dd05a36c54103acb88a9fb8b8f687" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">Image Generation</span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/b6b89e0254b740d38029ef3041f922b1" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffbf77942-b393-4936-a1d2-153c1933839c%2FUntitled.png?width=640&amp;table=block&amp;id=b6b89e02-54b7-40d3-8029-ef3041f922b1"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffbf77942-b393-4936-a1d2-153c1933839c%2FUntitled.png?width=640&amp;table=block&amp;id=b6b89e02-54b7-40d3-8029-ef3041f922b1" style="width:640px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h2 id="https://www.notion.so/e6ecbd7fd24643ad9e9ec76717849a91" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/e6ecbd7fd24643ad9e9ec76717849a91"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Introduction</span></span></h2><div id="https://www.notion.so/06876652511f4a79a0582c3849b1c2d0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Generative networks can be used to generate realistic images after being trained on sample images. For example, if we want to generate new images of dogs, we can train a GAN on thousands of samples of¬†images of dogs. Once the training has finished, the generator network will be able to generate new images that are different from the images in the training set. Image generation is used in marketing, logo generation, entertainment, social media, and so on. In the next chapter, we will be generating faces of anime characters.</span></span></p></div><h3 id="https://www.notion.so/4ddf098282364d7a977c17c27a07df7f" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/4ddf098282364d7a977c17c27a07df7f"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Latent Space Representation</span></span></h3><div id="https://www.notion.so/f26e99604cae474f9cdb3183a26db5f8" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F3f5caf06-bab4-4dff-bce7-8a87e43ff2c9%2FUntitled.png?width=486&amp;table=block&amp;id=f26e9960-4cae-474f-9cdb-3183a26db5f8"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F3f5caf06-bab4-4dff-bce7-8a87e43ff2c9%2FUntitled.png?width=486&amp;table=block&amp;id=f26e9960-4cae-474f-9cdb-3183a26db5f8" style="width:486px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/150a2d305a284402af74d17c6a62aaa8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/c9c701f9d35f4d4a9655ffdf7f1b5c60" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/c9c701f9d35f4d4a9655ffdf7f1b5c60"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">CONCEPT VECTORS FOR IMAGE EDITING</span></span></h3><div id="https://www.notion.so/c04490b0109f47ca845f2c48eec388e4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Given a latent space of representations, or an embedding space, certain directions in the space may encode interesting axes of variation in the original data. In a latent space of images of faces, for instance, there may be a smile vector, such that if latent point z is the embedded representation of a certain face, then latent point z + s is the embedded representation of the same face, smiling.</span></span></p></div><div id="https://www.notion.so/e3bbdce4ba224ecfa20391881202a431" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1e81abb6-e4bf-435b-bcf5-e1162dab8d63%2FUntitled.png?width=539&amp;table=block&amp;id=e3bbdce4-ba22-4ecf-a203-91881202a431"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1e81abb6-e4bf-435b-bcf5-e1162dab8d63%2FUntitled.png?width=539&amp;table=block&amp;id=e3bbdce4-ba22-4ecf-a203-91881202a431" style="width:539px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/8df8a390a258447c88484bdd6d9b6e1b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">There are concept vectors for essentially any independent dimension of variation in image space‚Äîin the case of faces, you may discover vectors for adding sunglasses to a face, removing glasses, turning a male face into as female face, and so on.</span></span></p></div><h3 id="https://www.notion.so/28461d76c0254913a4f2cfbfe2cfb8f4" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/28461d76c0254913a4f2cfbfe2cfb8f4"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Variational Autoencoder (VAE)</span></span></h3><div id="https://www.notion.so/e445c387cf07480dac4b2ab618f5f6c1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">A VAE turns the image into the parameters of a statistical distribution: a mean and a variance. Essentially, this means you‚Äôre assuming the input image has been generated by a statistical process, and that the randomness of this process should be taken into account during encoding and decoding. The VAE then uses the mean and variance parameters to randomly sample one element of the distribution, and decodes that element back to the original input. The stochasticity of this process improves robustness and forces the latent space to encode meaningful representations everywhere: every point sampled in the latent space is decoded to a valid output.</span></span></p></div><div id="https://www.notion.so/e30d2e11a11a406d9a734338f4ea87eb" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F73854a1b-4553-4655-8aab-8c05352e0d5d%2FUntitled.png?width=549&amp;table=block&amp;id=e30d2e11-a11a-406d-9a73-4338f4ea87eb"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F73854a1b-4553-4655-8aab-8c05352e0d5d%2FUntitled.png?width=549&amp;table=block&amp;id=e30d2e11-a11a-406d-9a73-4338f4ea87eb" style="width:549px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/b152f336f6da48d3bea8220c17ab4b2f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The parameters of a VAE are trained via two loss functions: a reconstruction loss that forces the decoded samples to match the initial inputs, and a regularization loss that helps learn well-formed latent spaces and reduce overfitting to the training data.</span></span></p></div><h2 id="https://www.notion.so/bb9ee585f75e4315b1ec6a0f74e4d754" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/bb9ee585f75e4315b1ec6a0f74e4d754"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Experiments</span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/91f417a84d7e4e2d816689caf5edf753" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.tensorflow.org/tutorials/generative/dcgan">TF 2.0 DCGAN for digit generation</a></span></span></li><li id="https://www.notion.so/02db63023f164041b5e326a1f5fa4b18" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.tensorflow.org/tutorials/generative/cvae">TF 2.0 VAE for digit generation</a></span></span></li><li id="https://www.notion.so/3671b3e299a24344ad152affcd539e5e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/PacktPublishing/Deep-Learning-with-PyTorch-1.x/blob/master/Chapter07/DCGAN.ipynb">Packt DCGAN</a></span></span></li><li id="https://www.notion.so/fbb54a692971466f8136e2e5138468f2" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/PacktPublishing/Hands-On-Neural-Network-Programming-with-TensorFlow/tree/master/Section%206">Packt Exercise</a></span></span></li><li id="https://www.notion.so/cce8e18076f54c63b64663512308ac65" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter13">Packt</a></span></span></li><li id="https://www.notion.so/7f65e7d4bca142f9b4910674f9869cab" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/ageron/handson-ml2/blob/master/17_autoencoders_and_gans.ipynb">Packt</a></span></span></li><li id="https://www.notion.so/0c5072dd80cd487a8a0998d67217a307" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb">BigGAN Colab</a></span></span></li><li id="https://www.notion.so/cbf910051e6346f28d9da349f1e2842b" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://colab.research.google.com/github/chainer-community/chainer-colab-notebook/blob/master/official_example_en/dcgan.ipynb">DCGAN Colab</a></span></span></li><li id="https://www.notion.so/c621a6bc4553419d9c8df505810853db" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_gans/tutorial_gans.ipynb">BigGAN MIT</a></span></span></li><li id="https://www.notion.so/dfdc1c99fbfa48bd83e0a625b716e199" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://colab.research.google.com/drive/1jsraJtjodGnJZoWF6O_rDxsQUgIrsslb">StyleGAN and BigGAN Colab</a></span></span></li></ul><h2 id="https://www.notion.so/cf1db2d73a894b4b957cae987eedb589" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/cf1db2d73a894b4b957cae987eedb589"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3D Object Generation</span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/04449445d75d42f7ba2138e1eb7de0a9" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Parametric 3D Exploration with Stacked Adversarial Networks, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/maxorange/pix2vox">[github]</a></span><span class="SemanticString">, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.youtube.com/watch?v=ITATOXVvWEM">[youtube]</a></span></span></li><li id="https://www.notion.so/2e1dc9caddfe4fc498029858906f5ada" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Learning a Probabilistic Latent Space of Object
Shapes via 3D Generative-Adversarial Modeling, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="http://papers.nips.cc/paper/6096-learning-a-probabilistic-latent-space-of-object-shapes-via-3d-generative-adversarial-modeling.pdf">[paper]</a></span><span class="SemanticString">, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/zck119/3dgan-release">[github]</a></span><span class="SemanticString">, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.youtube.com/watch?v=HO1LYJb818Q">[youtube]</a></span></span></li><li id="https://www.notion.so/0e07e9b4379547f49a7ab87f282fbf13" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">3D Shape Induction from 2D Views of Multiple Objects, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/1612.05872.pdf">[paper]</a></span></span></li><li id="https://www.notion.so/4a81bd0ecaab4b5898c29510f1eebcd6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Fully Convolutional Refined Auto-Encoding Generative Adversarial Networks for 3D Multi Object Scenes, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/yunishi3/3D-FCR-alphaGAN">[github]</a></span><span class="SemanticString">, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://becominghuman.ai/3d-multi-object-gan-7b7cee4abf80">[blog]</a></span></span></li></ul></div></details><details id="https://www.notion.so/582703b473ad409bad99b151c4381c20" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">3D Pose Estimation</span></span></summary><div class="Toggle__Content"><h2 id="https://www.notion.so/0451fed22443444b92d0146e7eefeb78" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/0451fed22443444b92d0146e7eefeb78"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">INTRODUCTION</span></span></h2><div id="https://www.notion.so/ef6a9f70a0d94947a4d78ec53e85fc54" class="Image Image--PageWidth"><figure><a href="https://github.com/facebookresearch/VideoPose3D/raw/master/images/demo_yt.gif"><img src="https://github.com/facebookresearch/VideoPose3D/raw/master/images/demo_yt.gif" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/867c2403b39e4dff8db64b31f0126b95" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">3D HPE has immediate applications in various tasks such as action understanding, surveillance, human-robot interaction, motion capture, and CGI.</span></span></p></div><div id="https://www.notion.so/675490dd14444cde92f94e68a0fdef29" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/f7c7e7ef977949f6a8b8820c9776c78a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/f7c7e7ef977949f6a8b8820c9776c78a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">BACKGROUND</span></span></h3><div id="https://www.notion.so/72536f7fc78a492d9781c67e25eebf0d" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F7df69b14-7c9a-43c6-b86f-be587708c4fb%2FUntitled.png?width=1920&amp;table=block&amp;id=72536f7f-c78a-492d-9781-c67e25eebf0d"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F7df69b14-7c9a-43c6-b86f-be587708c4fb%2FUntitled.png?width=1920&amp;table=block&amp;id=72536f7f-c78a-492d-9781-c67e25eebf0d" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/0adf8ce5e3e348b2a43493b388328dd6" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F48abd20f-2280-4690-bb4e-165f1607f94e%2FUntitled.png?width=1921&amp;table=block&amp;id=0adf8ce5-e3e3-48b2-a434-93b388328dd6"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F48abd20f-2280-4690-bb4e-165f1607f94e%2FUntitled.png?width=1921&amp;table=block&amp;id=0adf8ce5-e3e3-48b2-a434-93b388328dd6" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/ec36941e95c34c72aaa615cca3a6a740" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h2 id="https://www.notion.so/507422265edc4b95b486c9bd84893101" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/507422265edc4b95b486c9bd84893101"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">METHODOLOGY</span></span></h2><div id="https://www.notion.so/4781e0934a434d3f8cee1b2a87ee1d9a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The simplest way to predict 3D joint locations is to train a network to directly regress them from an image. </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="http://visal.cs.cityu.edu.hk/static/pubs/conf/accv14-3dposecnn.pdf">Li &amp; Chan</a></span><span class="SemanticString"> were the first to show that deep neural networks can be applied to 3D human pose estimation from single images. The framework consists of two types of tasks: 1) A joint point regression task; and 2) Joint point detection tasks. The input for both tasks is the bounding box images containing human subjects. The goal of the regression task is to estimate the positions of joint points relative to the root joint position. The aim of each detection task is to classify whether one local window contains a specific joint or not.</span></span></p></div><div id="https://www.notion.so/9d52034a26cc450d97833f9d51926220" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><details id="https://www.notion.so/ba3a08c09ad44d51bd5d7c82d3908418" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">architecture</span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/9a147652daad433886cac6a691793a92" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa29edf3b-067d-4bba-975c-c311ecae2143%2FUntitled.png?width=760&amp;table=block&amp;id=9a147652-daad-4338-86ca-c6a691793a92"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa29edf3b-067d-4bba-975c-c311ecae2143%2FUntitled.png?width=760&amp;table=block&amp;id=9a147652-daad-4338-86ca-c6a691793a92" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div></div></details><div id="https://www.notion.so/6a50cc307c7743a59f4e57e3e89e5a61" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1384fb40648d48239e81521b677c2a1e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The network is trained within a multi-task learning framework. As in, the features in the lower layers are to be shared between the regression and detection tasks during joint training. During the training, the gradients from both networks will be back-propagated to the same shared feature network, i.e., the network with layers from conv1 to pool3. In this case, the shared network tends to learn features that will benefit both tasks.</span></span></p></div><div id="https://www.notion.so/0ccbd14be9c34d63af61ecd874d73f8d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/343df9a845fa495da66f794f9b62e955" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Instead of directly predicting 3D Pose from images, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/1612.06524.pdf">Chen and Ramanan</a></span><span class="SemanticString"> explored a simple architecture that reasons through intermediate 2D pose predictions. It was based on two key observations - 1) Deep neural nets have revolutionized 2D pose estimation producing very good results and 2) 3D mocap data were readily available, making it easier to ‚Äúlift‚Äù predicted 2D poses to 3D through simple memorization.</span></span></p></div><div id="https://www.notion.so/d7b6b2270216499eb9a2570afb809c45" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Feec5beab-313e-43e8-a82f-a44bf0580992%2FUntitled.png?width=436&amp;table=block&amp;id=d7b6b227-0216-499e-b9a2-570afb809c45"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Feec5beab-313e-43e8-a82f-a44bf0580992%2FUntitled.png?width=436&amp;table=block&amp;id=d7b6b227-0216-499e-b9a2-570afb809c45" style="width:436px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/9871fe49375340a6867560864cf0993c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/43393da6e2ed46f2bdbc0281152bf161" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Given a 3D pose library (essentially a collection of 3D poses), they generate a large number of 2D projections. Given this training set of paired (2D,3D) data and predictions from a 2D pose estimation algorithm, the depths from the 3D pose associated with the closest matching 2D example from the library are returned.</span></span></p></div><div id="https://www.notion.so/938f66ccc1c34ab98f599cfa0a7739ef" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/b7fa9b794b8f4615bf24abbc74641ee8" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff33eab0d-31cf-4846-8812-b91867da02e3%2FUntitled.png?width=992&amp;table=block&amp;id=b7fa9b79-4b8f-4615-bf24-abbc74641ee8"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff33eab0d-31cf-4846-8812-b91867da02e3%2FUntitled.png?width=992&amp;table=block&amp;id=b7fa9b79-4b8f-4615-bf24-abbc74641ee8" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/d2fc77ed408242f9afececebe912a92e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/e65f3f546ef244f89bf377faab1d89c3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/1704.02447.pdf">Zhou et. al.</a></span><span class="SemanticString"> argued that sequential pipelines (like the previous paper) are sub-optimal because the original in-the-wild 2D image information, which contains rich cues for 3D pose recovery, is discarded in the second step. They proposed a weakly-supervised and end-to-end method that uses mixed 2D and 3D labels in a deep neural network that presents a two-stage cascaded structure. This approach leverages both 2D joint locations as well as intermediate feature representations from the original image.</span></span></p></div><div id="https://www.notion.so/3cc0868774a44ad8b3685ff191d227d4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/4f0024d5e7db40bb9e51755b7307313d" class="Divider"></div><h2 id="https://www.notion.so/798a299d03b142d2b66a81f7df62abfb" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/798a299d03b142d2b66a81f7df62abfb"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">EXPERIMENTS</span></span></h2><h3 id="https://www.notion.so/6a1c21e054b84c6f93adf61bf5aa94b7" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/6a1c21e054b84c6f93adf61bf5aa94b7"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">OpenVINO Model</span></span></h3><div id="https://www.notion.so/24409bf4316a470bab5b54029f4dc6f0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Link - </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://docs.openvinotoolkit.org/latest/omz_demos_python_demos_human_pose_estimation_3d_demo_README">https://docs.openvinotoolkit.org/latest/omz_demos_python_demos_human_pose_estimation_3d_demo_README</a></span></span></p></div><div id="https://www.notion.so/4e522913196f409db490942dc01518e3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/eb1baae2ed8048b9b29136a1c71f0ea2" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/eb1baae2ed8048b9b29136a1c71f0ea2"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">PyTorch implementation for 3D human pose estimation</span></span></h3><div id="https://www.notion.so/fb6d6855f7814f858df365587dd3ebff" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Git - </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/xingyizhou/Pytorch-pose-hg-3d">https://github.com/xingyizhou/Pytorch-pose-hg-3d</a></span></span></p></div><div id="https://www.notion.so/20c2f1760bdf482cb205eab2357a00ec" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/e417531064094a3ab9defa9d4946786a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/e417531064094a3ab9defa9d4946786a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">FAIR VideoPose3D Inference</span></span></h3><div id="https://www.notion.so/03357fd4f07d48609f4fc1c8d999865e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Git - </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/facebookresearch/VideoPose3D">https://github.com/facebookresearch/VideoPose3D</a></span></span></p></div><div id="https://www.notion.so/56e581d6042242949674a73f8ebf365c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><details id="https://www.notion.so/beb51e5315ce4dfdbddf91c808e561cc" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">3D Pose Estimation on Videos with VIBE</span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/fcb1236499b743daa659a0019b6bb9f0" class="ColorfulBlock ColorfulBlock--BgPurple Callout"><div class="Callout__Icon"><div class="Icon">‚òù</div></div><p class="Callout__Content"><span class="SemanticStringArray"><span class="SemanticString">VIBE: Video Inference for Human Body Pose and Shape Estimation</span></span></p></div><div id="https://www.notion.so/ea299547967b4ed69a1103dd43208ad9" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/806b5299f8fb4e098f84f16e95e21748" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">VIBE is a video pose and shape estimation method. It defines a novel temporal network architecture with a self-attention mechanism and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. </span></span></p></div><div id="https://www.notion.so/fc0ff11043614276a24425cb8487da76" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/d312a5cf591543f2a8edbd2abb7b18c2" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">A significant reason behind this is the lack of in-the-wild ground-truth 3D annotations, which are non-trivial to obtain even for single images. VIBE leverages the large-scale 3D motion-capture dataset called AMASS, which is sufficiently rich to learn a model of how people move. It learns to estimate sequences of 3D body shapes poses from in-the-wild videos such that a discriminator cannot tell the difference between the estimated motions and motions in the AMASS dataset. Check </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/1912.05656.pdf">paper</a></span><span class="SemanticString"> and </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/mkocabas/VIBE">git</a></span><span class="SemanticString"> to know more.</span></span></p></div><div id="https://www.notion.so/1fd9f7d4fb3747048a9f43ca16506837" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/cb33d94a567f45dda146f08d5f7f6685" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F055931bf-4459-4697-891d-42293d5eaccc%2FUntitled.png?width=1205&amp;table=block&amp;id=cb33d94a-567f-45dd-a146-f08d5f7f6685"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F055931bf-4459-4697-891d-42293d5eaccc%2FUntitled.png?width=1205&amp;table=block&amp;id=cb33d94a-567f-45dd-a146-f08d5f7f6685" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/d8182c1784024a3481a6d94f07ccd9cd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div></div><div id="https://www.notion.so/cace1c99d7e24ce58642c3d445e5e2b6" class="ColumnList"><div id="https://www.notion.so/9ad3725e69dd4c5d87c405b5a1515b35" class="Column" style="width:calc((100% - var(--column-spacing) * 1) * 0.5)"><div id="https://www.notion.so/4fc6a4a9c91241ed84ef598e41e2d0f9" class="Video"><div class="Video__Content"><video width="400" controls=""><source src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fafe4944e-e6d5-4eea-8a66-9ea90ff22f9b%2Fsample_video.mp4?table=block&amp;id=4fc6a4a9-c912-41ed-84ef-598e41e2d0f9"/></video></div><p class="Video__Caption"><span class="SemanticStringArray"><span class="SemanticString">This is a 10-sec input video</span></span></p></div><div id="https://www.notion.so/4d55906c0b0d4e7f916464a4a57920e4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div></div><div id="https://www.notion.so/004df3a268fd493f9f8149f0dd074138" class="Column" style="width:calc((100% - var(--column-spacing) * 1) * 0.5)"><div id="https://www.notion.so/9d266ef9089f4487ad758cb2756fd51f" class="Video"><div class="Video__Content"><video width="400" controls=""><source src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6675db5f-6864-4701-98d7-dc81165cb676%2Fsample_video_vibe_result.mp4?table=block&amp;id=9d266ef9-089f-4487-ad75-8cb2756fd51f"/></video></div><p class="Video__Caption"><span class="SemanticStringArray"><span class="SemanticString">This is the output 3-d structure</span></span></p></div></div></div></div></details><div id="https://www.notion.so/d4baa7594c504e29a756079a9b5871ab" class="Divider"></div><h2 id="https://www.notion.so/9dbd086a9ce744dbaa1f3d9aaa4012a5" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/9dbd086a9ce744dbaa1f3d9aaa4012a5"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">RESULTS &amp; DISCUSSION</span></span></h2><div id="https://www.notion.so/f3e14b8efed741c0a4e379b5d4a0aa3f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Current computer vision algorithms and deep learning-based methods can detect people in images and estimate their 2D pose with a remarkable accuracy. However, understanding humans and estimating their pose and shape in 3D is still an open problem. The ambiguities in lifting 2D pose to 3D, the lack of annotated data to train 3D pose regressors in the wild and the absence of a reliable evaluation dataset in real world situations make the problem very challenging.</span></span></p></div><div id="https://www.notion.so/568743c43ab24b79bdc884a4a30bb04f" class="Divider"></div><h2 id="https://www.notion.so/d9c9901da3ce427c84a396e5b3f75f44" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/d9c9901da3ce427c84a396e5b3f75f44"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">REFERENCES</span></span></h2><ol class="NumberedListWrapper"><li id="https://www.notion.so/23df55affce343c092fa3b082732cb29" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://nanonets.com/blog/human-pose-estimation-3d-guide/">S. C. Babu, Nanonets.com. &quot;A 2019 guide to 3D Human Pose Estimation&quot;, 2019.</a></span></span></li></ol><div id="https://www.notion.so/24a09c42a703438a8ed5bf64b6c099de" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/ebc59e1f0d60436599c46ac32e3142f8" class="Divider"></div></div></details><details id="https://www.notion.so/01568320a7524d38bd531b54737a7d40" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">Optical Character Recognition</span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/5ed402ab74004a8581e57fede4bef8be" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F35ec3fc3-5b83-42fb-8cf4-04a56d11934f%2FUntitled.png?width=680&amp;table=block&amp;id=5ed402ab-7400-4a85-81e5-7fede4bef8be"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F35ec3fc3-5b83-42fb-8cf4-04a56d11934f%2FUntitled.png?width=680&amp;table=block&amp;id=5ed402ab-7400-4a85-81e5-7fede4bef8be" style="width:680px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/da2e0e63463f464a81066462a47270aa" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/a2a512d27d444b81b5376fed6ae356ed" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Optical Character Recognition(OCR) has been an active area of research in AI and Computer Vision. It enables us to Recognize(identify) text from images or PDF. OCR has a variety of applications like Data Entry for Business, Number Plate Recognition, Automated Passport Recognition, Quick Document Verification, IoT Application, Task Automation, etc. This Application has the potential to increase revenue and save time.</span></span></p></div><div id="https://www.notion.so/92b76e8bc7434d87ab55ce824b56b656" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/412d63ab3a4447bf8995f89667c42b9c" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/412d63ab3a4447bf8995f89667c42b9c"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Bag of Tricks</span></span></h3><div id="https://www.notion.so/1a440c5271a84db89d900d07792a6e6b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">For Blur, Noisy and colorful image we need to follow some image-processing steps like making image black and white, remove salt and pepper noise using lowpass filters such as averaging filters or Gaussian Filter, We can also make blur image sharpen by using Highpass filter such as Sobel filters. This Image Processing operation can also be implemented by the OpenCV library in python.</span></span></p></div><div id="https://www.notion.so/09eb385a250b456797165c56eff8a2e1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/128433eb46ae4765900c490d73c5ba12" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/128433eb46ae4765900c490d73c5ba12"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Tools</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/1422eabc01cb4163b63c1f2058081416" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Keras-OCR</span></span></li></ul><details id="https://www.notion.so/77c3200c1dd1434fa29f70bb9e37cb14" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">Tesseract</span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/09a35a12b7ee4719b4712c534cd8a396" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Pros:</strong></span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/317cbe7393c04b49bd8be3fd7b59941e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Easy to use</span></span></li><li id="https://www.notion.so/20aa674141d544b6af7b569ce02d1ada" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Fast Detection</span></span></li><li id="https://www.notion.so/49aa5d45d19f41fab37c799d01209fb3" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Most Popular</span></span></li><li id="https://www.notion.so/c37855313d6d4d80abff901fcdd8fa40" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Most efficient</span></span></li><li id="https://www.notion.so/035ca6f5d5f7473689ed33df106a97c5" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Support 100+ Language</span></span></li><li id="https://www.notion.so/c78d5fba6295470887405f57ee10e454" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Oldest OCR Library</span></span></li><li id="https://www.notion.so/4d12acbf2710431ba7311d678042a8a1" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Command-line support</span></span></li></ul><div id="https://www.notion.so/e609e1490e4e42ebb98e0990650cc8a8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Cons:</strong></span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/1cd273334b1b4b4b86197b575bed7729" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Only works on CPU</span></span></li><li id="https://www.notion.so/dc090d0423d748e3845f190e0b08c4a2" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Doesn‚Äôt perform well on Blur, Noisy and colorful image</span></span></li><li id="https://www.notion.so/ae28856f12a04758a2fac3dfca25df77" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Performance decrease for lower font size in low-resolution images</span></span></li><li id="https://www.notion.so/fde735c864db42478926ae0f63caa470" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Doesn‚Äôt work well on complex Forms</span></span></li></ul></div></details><details id="https://www.notion.so/7e9620019a70456898575af4a51f960c" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">EasyOCR</span></span></summary><div class="Toggle__Content"><div id="https://www.notion.so/8b64df80fd69485c8441d4fc44528b79" class="Bookmark"><a href="https://github.com/JaidedAI/EasyOCR"><h5 class="Bookmark__Title">JaidedAI/EasyOCR</h5><p class="Bookmark__Desc">Ready-to-use OCR with 70+ languages supported including Chinese, Japanese, Korean and Thai. 17 November 2020 - Version 1.2 New language supports for Telugu and Kannada. These are experimental lite recognition models. Their file sizes are only around 7% of other models and they are ~6x faster at inference with CPU.</p><p class="Bookmark__Link">https://github.com/JaidedAI/EasyOCR</p></a></div><div id="https://www.notion.so/19ae93b442004effa0f912e1c7ccd8f5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">It&#x27;s one of the best open-source Multilingual libraries for OCR. It supports 70+ languages currently and more will be added soon. Due to the open-source nature and python support, it&#x27;s easy to add new languages in Easy OCR. It is Built on top of PyTorch, ResNet, CTC, and beam-search-based decoder.</span></span></p></div></div></details><details id="https://www.notion.so/404a96cbc1354828bb955ee74fa3b223" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle Toggle--Empty"><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">ArabicOCR</span></span></summary><div class="Toggle__Content"></div></details></div></details><div id="https://www.notion.so/5305212aa91a4e48940f57f552c75a99" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/7d2a776952b446abac9e53cbcd132813" class="Divider"></div><h2 id="https://www.notion.so/49d9506f3be44f8c91b8acd54f308952" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/49d9506f3be44f8c91b8acd54f308952"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">üöÄ Live Apps</span></span></h2><details id="https://www.notion.so/862ca579b453441293daeab76b7bc582" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">Mushroom Classification - </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://share.streamlit.io/sparsh-ai/streamlit-2d900058/app.py">Play with it!</a></span></span></summary><div class="Toggle__Content"><ul class="BulletedListWrapper"><li id="https://www.notion.so/344901d239cc427a9293c18c12b82041" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 1: Project Overview and Demo</span></span></li><li id="https://www.notion.so/94a90e1d73834211b8c33d49e4be6aa6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 2: Turn Simple Python Scripts into Web Apps</span></span></li><li id="https://www.notion.so/4754173e928c40f98ba2ca05e1343d74" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 3: Load the Mushrooms Data Set</span></span></li><li id="https://www.notion.so/40ee2eb7687d40e481608c8f83371d0f" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 4: Creating Training and Test Sets</span></span></li><li id="https://www.notion.so/3b95e46f6dd0441999be840ebb352960" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 5: Plot Evaluation Metrics</span></span></li><li id="https://www.notion.so/fd2660f6a4c24432b92f4d31ec049296" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 6: Training a Support Vector Classifier</span></span></li><li id="https://www.notion.so/7848e8a1d3134409bf9253a599e16c16" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 7: Training a Support Vector Classifier (Part 2)</span></span></li><li id="https://www.notion.so/eb6e5888acfa4e0fa4fd5060a091a84f" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 8: Train a Logistic Regression Classifier</span></span></li><li id="https://www.notion.so/4a0b1af9ff3741e8a29a1128849a2a5d" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 9: Training a Random Forest Classifier</span></span></li></ul></div></details><details id="https://www.notion.so/df10679e4cdc40b4bc2db68c777f1091" class="ColorfulBlock ColorfulBlock--ColorDefault Toggle "><summary class="Toggle__Summary"><span class="SemanticStringArray"><span class="SemanticString">Twitter US Airline Sentiment - </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://share.streamlit.io/sparsh-ai/streamlit-05af01d0/app.py">Play with it!</a></span></span></summary><div class="Toggle__Content"><ul class="BulletedListWrapper"><li id="https://www.notion.so/29956d0c01684f1c9cdab0bc380e8a29" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 1: Project Overview and Demo</span></span></li><li id="https://www.notion.so/e2327bcfa10448369ef5b454ed919ff5" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 2: Turn Simple Python Scripts into Web Apps</span></span></li><li id="https://www.notion.so/8924375a46f349029518aa71cf0f2bb9" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 3: Load the Twitter US Airline Sentiment Data</span></span></li><li id="https://www.notion.so/9e6fc8a2440744bca529297d6478527e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 4: Display Tweets in the Sidebar</span></span></li><li id="https://www.notion.so/2531930325f24936ab1a557747722eb1" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 5: Plot Interactive Bar Plots and Pie Charts</span></span></li><li id="https://www.notion.so/cfb5fb5cfaf443f28344c6a823b94187" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 6: Plotting Location Data on an Interactive Map</span></span></li><li id="https://www.notion.so/a5ba9b6373c34254b1a50a11fd899566" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 7: Plot Number of Tweets by Sentiment for Each Airline</span></span></li><li id="https://www.notion.so/6393eec4515c4778b2a3ad3884a1b4f5" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Task 8: Word Cloud for Positive, Neutral, and Negative Tweets</span></span></li></ul></div></details><div id="https://www.notion.so/562c2bf70a5c4ad4a738206668d177cc" class="Divider"></div><h2 id="https://www.notion.so/0065c52d80314500aa2266214217e5c4" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/0065c52d80314500aa2266214217e5c4"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">üéûÔ∏èVideos</span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/177969bfea804ebba2389b6f0edaece8" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">AI overview slides [</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://youtu.be/wF8UpcSGTT0">link</a></span><span class="SemanticString">]</span></span></li></ul></article>  <footer class="Footer">
        <div>&copy; Sparsh Agarwal 2021</div>
        <div>&centerdot;</div>
        <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
            rel="noopener noreferrer">Notablog</a>.
        </div>
    </footer>
</body>

</html>