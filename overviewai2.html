<!DOCTYPE html>
<html lang="en">

<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5F9Y9HC95G"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5F9Y9HC95G');
</script>
<script data-ad-client="ca-pub-8325812071117232" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <!-- Chrome, Firefox OS and Opera Status Bar Color -->
  <meta name="theme-color" content="#FFFFFF">
  <meta property="og:title" content="AI Overview Series - Part 2">
    <meta name="description" content="Frameworks, Tools, Models and Use¬†cases">
  <meta property="og:description" content="Frameworks, Tools, Models and Use¬†cases">
    <meta property="og:type" content="blog">
  <title>AI Overview Series - Part 2</title>
<link rel="icon" type="image/png" href="/favicon.png"/>
  <!-- Favicon -->
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
  <link rel="stylesheet" type="text/css"
    href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
  <link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
  <link rel="stylesheet" type="text/css" href="css/notablog.css">
  <link rel="stylesheet" type="text/css" href="css/theme.css">
  <style>
    :root {
      font-size: 18px;
    }

    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
    <a href="index.html">
      <div class="Navbar__Btn"> <span>Home</span></div>
    </a>
                                                                                                                                    <span class="Navbar__Delim">&centerdot;</span>
    <a href="extras.html">
      <div class="Navbar__Btn"> <span>Extras</span></div>
    </a>
                <span class="Navbar__Delim">&centerdot;</span>
    <a href="about.html">
      <div class="Navbar__Btn"> <span>About</span></div>
    </a>
                <span class="Navbar__Delim">&centerdot;</span>
    <a href="resources.html">
      <div class="Navbar__Btn"> <span>Resources</span></div>
    </a>
          </nav>
  <header class="Header">
          <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
        <h1 class="Header__Title">AI Overview Series - Part 2</h1>
            <div class="DateTagBar">
                <span class="DateTagBar__Item DateTagBar__Date">Posted on Sat, Jan 9, 2021</span>
                          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--gray">
            <a href="tag/pose estimation.html">pose estimation</a>
          </span>
                <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--blue">
            <a href="tag/face recognition.html">face recognition</a>
          </span>
                <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--red">
            <a href="tag/object tracking.html">object tracking</a>
          </span>
                  </div>
          </header>
      <article id="https://www.notion.so/8103a1cef00444c2a83f00cfb8400105" class="PageRoot"><div id="https://www.notion.so/6d93db6ef23048ef93935c305947f7d3" class="ColorfulBlock ColorfulBlock--BgBlue Callout"><div class="Callout__Icon"><div class="Icon">üìù</div></div><p class="Callout__Content"><span class="SemanticStringArray"><span class="SemanticString">Frameworks, Tools, Models and Use¬†cases</span></span></p></div><h1 id="https://www.notion.so/eb2987b7289f48c1a901af47eed67d2c" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/eb2987b7289f48c1a901af47eed67d2c"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Pose Estimation</span></span></h1><div id="https://www.notion.so/afc969abe1ce4ebfa8366161902de649" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4d7e60a8-0fa6-4801-94e2-e48dc42e668f%2Fimg.png?table=block&amp;id=afc969ab-e1ce-4ebf-a836-6161902de649"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4d7e60a8-0fa6-4801-94e2-e48dc42e668f%2Fimg.png?table=block&amp;id=afc969ab-e1ce-4ebf-a836-6161902de649" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/3fba34fa76e04201aef580819c88ab7d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Pose estimation is a computer vision task that infers the pose of a person or object in an image or video. This is typically done by identifying, locating, and tracking the number of key points on a given object or person. For objects, this could be corners or other significant features. And for humans, these key points represent major joints like an elbow or knee.</em></span></span></p></div><div id="https://www.notion.so/efbba9e7564c4efebaf7494b414a9b04" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Applications </strong></span><span class="SemanticString">- Activity recognition, motion capture, fall detection, plank pose corrector, yoga pose identifier, body ration estimation</span></span></p></div><div id="https://www.notion.so/c501f8f911c54aa0a25f3f62dffc3a79" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Scope </strong></span><span class="SemanticString">- 2D skeleton map (3D mapping coming soon), Human Poses (product poses coming soon), Single and Multi-pose, Real-time</span></span></p></div><div id="https://www.notion.so/596ffc5f1c75491095445fc8ba9e3752" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Tools </strong></span><span class="SemanticString">- Tensorflow PoseNet API</span></span></p></div><h2 id="https://www.notion.so/5132ee8cb7624baaa8af0743f21b3d42" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/5132ee8cb7624baaa8af0743f21b3d42"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Models</strong></span></span></h2><h3 id="https://www.notion.so/07b405f18b2b455693551347b04616a6" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/07b405f18b2b455693551347b04616a6"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">OpenPose</span></span></h3><div id="https://www.notion.so/2272faba0add40efbfd0a5fcdda3aff7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1812.08008"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. arXiv, 2016.</em></a></span></span></p></div><div id="https://www.notion.so/f73929278500427e8992127ee0afe97c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">A standard bottom-up model that supports real-time multi-person 2D pose estimation. The authors of the paper have shared two models‚Ää‚Äî‚Ääone is trained on the Multi-Person Dataset ( MPII ) and the other is trained on the COCO dataset. The COCO model produces 18 points, while the MPII model outputs 15 points.</span></span></p></div><h3 id="https://www.notion.so/3b4d49fe281e444e8a92cfbe7d3c62f3" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/3b4d49fe281e444e8a92cfbe7d3c62f3"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">PoseNet</span></span></h3><div id="https://www.notion.so/7a58a90938ca4b898848c51f10bea2e7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">PoseNet is a machine learning model that allows for Real-time Human Pose Estimation. PoseNet can be used to estimate either a single pose or multiple poses PoseNet v1 is trained on MobileNet backbone and v2 on ResNet backbone.</span></span></p></div><h2 id="https://www.notion.so/1ef78217c48c44e38f68663ec30b6450" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/1ef78217c48c44e38f68663ec30b6450"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Use Cases</strong></span></span></h2><ol class="NumberedListWrapper"><li id="https://www.notion.so/ff57271272a74104a09d1ec738393744" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">OpenPose Experiments - Four types of experiments with pre-trained OpenPose model‚Ää‚Äî‚ÄäSingle and Multi-Person Pose Estimation with OpenCV, Multi-Person Pose Estimation with PyTorch and Pose Estimation on Videos. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Pose-Estimation-with-OpenPose-7f01bee1534243f3836728d03a419969">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/286fe2285d144226aefa991ee47d464f" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">Pose Estimation Inference Experiments - Experimented with pre-trained pose estimation models. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Pose-Estimation-with-OpenPifPaf-8cb982455e01478e876c52e9324d8e6b">this</a></span><span class="SemanticString"> notion for experiments with the OpenPifPaf model, </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Pose-Estimation-with-Keypoint-RCNN-in-TorchVision-96e6aad0f36f44d3bff28e60525c6d31">this</a></span><span class="SemanticString"> one for the TorchVision Keypoint R-CNN model, and </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Detectron-2-bb7f769860fa434d923feef3a99f9cbb">this</a></span><span class="SemanticString"> notion for the Detectron2 model.</span></span></li><li id="https://www.notion.so/ae9a3d8802ed48b6af44ca54c573ced5" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">Pose Detection on the Edge - Train the pose detector using Teachable machine, employing the PoseNet model (multi-person real-time pose estimation) as the backbone and serve it to the web browser using ml5.js. This system will infer the end-users pose in real-time via a web browser. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://teachablemachine.withgoogle.com/train/pose">this</a></span><span class="SemanticString"> link and </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/ml5-js-Pose-Estimation-with-PoseNet-5661cefe46b449998cc31838441dc26a">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/8303c417d6bb422f8bc338a64b2400a9" class="NumberedList" value="4"><span class="SemanticStringArray"><span class="SemanticString">Pose Detection on the Edge using OpenVINO - Optimize the pre-trained pose estimation model using the OpenVINO toolkit to make it ready to serve at the edge (e.g. small embedded devices) and create an OpenVINO inference engine for real-time inference. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/OpenVINO-4c4fc4f167cc4601ade5795a241a60da">this</a></span><span class="SemanticString"> notion.</span></span></li></ol><div id="https://www.notion.so/34e0fd6486064ae89ed7d62400d5975c" class="Divider"></div><h1 id="https://www.notion.so/482dfa45365040e38d364d66a6a31cbe" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/482dfa45365040e38d364d66a6a31cbe"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Facial Analytics</span></span></h1><div id="https://www.notion.so/e965ab79644e469192c37ca9f04aab5d" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F9d277f5c-925a-4096-84d0-86dfab2c9791%2Fimg.png?table=block&amp;id=e965ab79-644e-4691-92c3-7ca9f04aab5d"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F9d277f5c-925a-4096-84d0-86dfab2c9791%2Fimg.png?table=block&amp;id=e965ab79-644e-4691-92c3-7ca9f04aab5d" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/7a43b75ae61844d6afabb7b059d7d4ea" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Analyze the facial features like age, gender, emotion, and identity.</em></span></span></p></div><div id="https://www.notion.so/8f313e3dfa414bd28f9bd37479a3ad73" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Applications </strong></span><span class="SemanticString">- Identity verification, emotion detection</span></span></p></div><div id="https://www.notion.so/41e38e86f489494ba6be32d259e18ed6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Scope </strong></span><span class="SemanticString">- Human faces only, Real-time</span></span></p></div><div id="https://www.notion.so/20ff217bb36d437cafed50f380df9a92" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Tools </strong></span><span class="SemanticString">- OpenCV, dlib</span></span></p></div><h2 id="https://www.notion.so/89736e78892b422eb42d5ff343bcb61d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/89736e78892b422eb42d5ff343bcb61d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Models</strong></span></span></h2><h3 id="https://www.notion.so/a4814695e0dd4fd0abf4c239cbd24b6d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/a4814695e0dd4fd0abf4c239cbd24b6d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">FaceNet</span></span></h3><div id="https://www.notion.so/09d44ee20ab542ce83281aafbbc60448" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://openaccess.thecvf.com/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">FaceNet: A Unified Embedding for Face Recognition and Clustering. CVPR, 2015.</em></a></span></span></p></div><h3 id="https://www.notion.so/644ed0f4092b4545b20228f2a3cf65da" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/644ed0f4092b4545b20228f2a3cf65da"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">RetinaFace</span></span></h3><div id="https://www.notion.so/a4f450e727d14c4f9ce8db9e38d37311" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1905.00641v2"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">RetinaFace: Single-stage Dense Face Localisation in the Wild. arXiv, 2019.</em></a></span></span></p></div><h3 id="https://www.notion.so/081458f6820542c9985e0409ac6b85c8" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/081458f6820542c9985e0409ac6b85c8"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">FER+</span></span></h3><div id="https://www.notion.so/e3eeda84a13a410f8768d90b16837ec1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1608.01041v2"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution. arXiv, 2016.</em></a></span></span></p></div><h2 id="https://www.notion.so/db153bfbd32e4b3f87972a4e368240aa" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/db153bfbd32e4b3f87972a4e368240aa"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Use Cases</strong></span></span></h2><ol class="NumberedListWrapper"><li id="https://www.notion.so/99d893343e5942b7ac204b6163313d0e" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">Automatic Attendance System via Webcam - We use Face Recognition library and OpenCV to create a real-time webcam-based attendance system that will automatically recognize the face and log an attendance into the excel sheet. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Face-Recognition-based-Automated-Attendance-System-dfb6f70527994ea4be11caf69b054350">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/8ddf6b25c0544acdbd66bf3837897a4b" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">Detectron2 Fine-tuning for face detection - Fine-tuned detectron2 on human face dataset to detect the faces in images and videos. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Detectron-2-bb7f769860fa434d923feef3a99f9cbb">this</a></span><span class="SemanticString"> notion.</span></span></li></ol><div id="https://www.notion.so/1d3ff3559e3b4d11818f7c6fe5035b74" class="Divider"></div><h1 id="https://www.notion.so/b0a1cd31b0c3451f995633724c835135" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/b0a1cd31b0c3451f995633724c835135"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Scene Text Recognition</span></span></h1><div id="https://www.notion.so/f100cdba02a74617a710d987d639880e" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb8f8d53f-dbef-41b0-a935-fcd9ad4a945d%2Fimg.png?table=block&amp;id=f100cdba-02a7-4617-a710-d987d639880e"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb8f8d53f-dbef-41b0-a935-fcd9ad4a945d%2Fimg.png?table=block&amp;id=f100cdba-02a7-4617-a710-d987d639880e" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/73b51a4c689f427e97e43bdc607b83b3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Text‚Ää‚Äî‚Ääas a fundamental tool of communicating information‚Ää‚Äî‚Ääscatters throughout natural scenes, e.g., street signs, product labels, license plates, etc. Automatically reading text in natural scene images is an important task in machine learning and gains increasing attention due to a variety of applications. For example, accessing text in images can help the visually impaired understand the surrounding environment. To enable autonomous driving, one must accurately detect and recognize every road sign. Indexing text in images would enable image search and retrieval from billions of consumer photos on the internet.</em></span></span></p></div><div id="https://www.notion.so/ec56844308504c41a664e2b6179db40b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Applications </strong></span><span class="SemanticString">- Indexing of multimedia archives, recognizing signs in driver assisted systems, providing scene information to visually impaired people, identifying vehicles by reading their license plates.</span></span></p></div><div id="https://www.notion.so/1c6202fe0e994f6ea300cc031711ec94" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Tools </strong></span><span class="SemanticString">- OpenCV, Tesseract, PaddleOCR</span></span></p></div><h2 id="https://www.notion.so/ca574a0392594caeab4d49eb0be0750c" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/ca574a0392594caeab4d49eb0be0750c"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Models</strong></span></span></h2><h3 id="https://www.notion.so/09ab8845ad094d8ca3e6951a6f04743f" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/09ab8845ad094d8ca3e6951a6f04743f"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Semantic Reasoning Networks</span></span></h3><div id="https://www.notion.so/7d2646d261264515826f83135ecba9e5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/2003.12294v1"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Towards Accurate Scene Text Recognition with Semantic Reasoning Networks. arXiv, 2020.</em></a></span></span></p></div><h3 id="https://www.notion.so/cb5be21705e7410183dafaec8140cbb1" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/cb5be21705e7410183dafaec8140cbb1"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Differentiable Binarization</span></span></h3><div id="https://www.notion.so/4372d7cebb8247d7b50d15c5b7153ea7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1911.08947v2"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Real-time Scene Text Detection with Differentiable Binarization. arXiv, 2019.</em></a></span></span></p></div><h3 id="https://www.notion.so/e83c71f598c549e1af4132140ad1b4d5" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/e83c71f598c549e1af4132140ad1b4d5"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">CRAFT</span></span></h3><div id="https://www.notion.so/7ec1a562a65145ad988719f7a871ab87" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1904.01941v1"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Character Region Awareness for Text Detection. arXiv, 2019.</em></a></span></span></p></div><h3 id="https://www.notion.so/0bc80f78e2ef49eba5a902e28eeb7a9d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/0bc80f78e2ef49eba5a902e28eeb7a9d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">EAST</span></span></h3><div id="https://www.notion.so/b27fa4c171f046b6a2cc93b8dc244974" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1704.03155v2"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">EAST: An Efficient and Accurate Scene Text Detector. arXiv, 2017.</em></a></span></span></p></div><h2 id="https://www.notion.so/3963f809f8324c10b7f4528d019af2d8" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/3963f809f8324c10b7f4528d019af2d8"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Use Cases</strong></span></span></h2><ol class="NumberedListWrapper"><li id="https://www.notion.so/7878787b653b40f986392a95ccf22429" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">Scene Text Detection with EAST Tesseract - Detect the text in images and videos using EAST model. Read the characters using Tesseract. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Scene-Text-Detection-with-EAST-Tesseract-583f882db70b43b5b3005d89ced8d8fd">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/3d1e58bcc3204ba48b6f8930e00bd990" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">Scene Text Recognition with DeepText - Detect and Recognize text in images with an end-to-end model named DeepText. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Scene-Text-Recognition-with-DeepText-3dbc00e6bdf548a3b8539be1adb8f2d5">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/106a10f2212d41e0a01eca9579b69ca2" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">Automatic License Plate Recognition - Read the characters on the license plate image using Tesseract OCR. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Automatic-License-Plate-Recognition-10ec22181b454b1facc99abdeadbf78f">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/7a214062cea74743a01dde98add5e3a9" class="NumberedList" value="4"><span class="SemanticStringArray"><span class="SemanticString">Keras OCR Toolkit Experiment - Keras OCR is a deep learning based toolkit for text recognition in images. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Keras-OCR-d15bff7629fa4fbf8d8a7fb21d2a69c5">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/1fc9e579eace4a05a38c83e58ff5f0e9" class="NumberedList" value="5"><span class="SemanticStringArray"><span class="SemanticString">OCR Experiments - Experiments with three OCR tools‚Ää‚Äî‚ÄäTesseract OCR, Easy OCR, and Arabic OCR. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/OCR-Simple-Experiments-a606ff9003b14de589073864c150aa81">this</a></span><span class="SemanticString"> and </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Optical-Character-Recognition-6eec9092cc70455a91dd92278e4677a8">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/ad0eedbaff4243bdbeaefebd6ac28f7b" class="NumberedList" value="6"><span class="SemanticStringArray"><span class="SemanticString">PaddleOCR Experiments - Experiments with state of the art lightweight and multi-lingual OCR. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Paddle-OCR-5ab56a38a594478da92314f246159193">this</a></span><span class="SemanticString"> notion.</span></span></li></ol><div id="https://www.notion.so/8d83d778a6664f9686f1cc62f4d0d920" class="Divider"></div><h1 id="https://www.notion.so/5a6da4e9596d4452b1bd981c1c8885c6" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/5a6da4e9596d4452b1bd981c1c8885c6"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Object Tracking</span></span></h1><div id="https://www.notion.so/00f5d9213cea4632bfea7f411e7a77b1" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff6d5203d-fccb-4a25-827a-5fa6b3d9a2f5%2Fimg.png?table=block&amp;id=00f5d921-3cea-4632-bfea-7f411e7a77b1"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff6d5203d-fccb-4a25-827a-5fa6b3d9a2f5%2Fimg.png?table=block&amp;id=00f5d921-3cea-4632-bfea-7f411e7a77b1" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/31b4245f38164372ad4e13f6382dbb0b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Object tracking is the process of 1) Taking an initial set of object detections (such as an input set of bounding box coordinates, 2) Creating a unique ID for each of the initial detections, and then 3) tracking each of the objects as they move around frames in a video, maintaining the assignment of unique¬†IDs.</em></span></span></p></div><div id="https://www.notion.so/590f3f769d674559ba85be1a0591b631" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Applications </strong></span><span class="SemanticString">- In-store consumer behavior tracking, Apply security policies like crowd management, traffic management, vision-based control, human-computer interface, medical imaging, augmented reality, robotics.</span></span></p></div><div id="https://www.notion.so/74ea125d8d364fe68ac0d8baea5b15f1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Scope </strong></span><span class="SemanticString">- Track objects in images and videos, 2-dimensional tracking, Bounding boxes and pixel masks, Single and Multiple Object Tracking</span></span></p></div><div id="https://www.notion.so/8e2755751e024e5eafea0b37f2926bea" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Tools </strong></span><span class="SemanticString">- Detectron2, OpenCV</span></span></p></div><h2 id="https://www.notion.so/f97ea5ffa9514820a630f00298ca8548" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/f97ea5ffa9514820a630f00298ca8548"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Models</strong></span></span></h2><h3 id="https://www.notion.so/337294ecc00149fe93bb8c7667d04eeb" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/337294ecc00149fe93bb8c7667d04eeb"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">FairMOT</span></span></h3><div id="https://www.notion.so/b8b2ffdb9e384e388fc465d1da28be95" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/2004.01888"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">On the Fairness of Detection and Re-Identification in Multiple Object Tracking. arXiv, 2020.</em></a></span></span></p></div><h3 id="https://www.notion.so/10713eabf71e412ba796b5b980668ec1" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/10713eabf71e412ba796b5b980668ec1"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">DeepSORT</span></span></h3><div id="https://www.notion.so/6973574843a74479a7d175829e385ede" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1703.07402"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Simple Online and Realtime Tracking with a Deep Association Metric. arXiv, 2017.</em></a></span></span></p></div><div id="https://www.notion.so/645c806609f0446ba87d73905a7256b2" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Detect object with models like YOLO or Mask R-CNN and then track using DeepSORT.</span></span></p></div><h3 id="https://www.notion.so/456c5cdcec004096a3219af56eb00b08" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/456c5cdcec004096a3219af56eb00b08"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">GOTURN</span></span></h3><div id="https://www.notion.so/6230124308da4e8297ced145c6c20519" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1604.01802"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Learning to Track at 100 FPS with Deep Regression Networks. arXiv, 2016.</em></a></span></span></p></div><div id="https://www.notion.so/ad4e59c2da8d4c4f9a5b046f96338bf5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">CNN offline learning tracker.</span></span></p></div><h3 id="https://www.notion.so/9cd2ad9b92c74c4ba22f7f3873f985ff" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/9cd2ad9b92c74c4ba22f7f3873f985ff"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">MDNet</span></span></h3><div id="https://www.notion.so/9d000f2689c8429fbdfda2f6b0be1b9e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1808.08834"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Real-Time MDNet. arXiv, 2018.</em></a></span></span></p></div><div id="https://www.notion.so/4e65cad36e83475e8a8ac14bdb05a1d6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">CNN online learning tracker.</span></span></p></div><h3 id="https://www.notion.so/3d1b3db9e78f427db6fce436d27717f9" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/3d1b3db9e78f427db6fce436d27717f9"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">ROLO</span></span></h3><div id="https://www.notion.so/8af1c4ff960b4f82ae39247fbb8664cb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1607.05781"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking. arXiv, 2016.</em></a></span></span></p></div><div id="https://www.notion.so/9fabbcd8db304c2ebb9059cdf49ba994" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">CNN + LSTM tracker.</span></span></p></div><h2 id="https://www.notion.so/2d7dfefe09714974b0d671add2c8162b" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/2d7dfefe09714974b0d671add2c8162b"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Use Cases</strong></span></span></h2><ol class="NumberedListWrapper"><li id="https://www.notion.so/cc82ca91347842b0b202b5c4e5fcdcad" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">Pedestrian Tracking - Pedestrian Tracking with YOLOv3 and DeepSORT. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Pedestrian-Tracking-with-YOLOv3-and-DeepSORT-a38ea37a2abf4755aacc691bd6b859a1">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/9393d2a61ffa4e87983c972a15e01dd6" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">Object Tracking - Object tracking with FRCNN and SORT. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Object-tracking-with-FRCNN-and-SORT-e555d6174d2e4c1e993526c89555f96b">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/c4c1552e6d684cbfa64f5daf6e83cb59" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">Object Tracking - Tested out 5 algorithms on videos‚Ää‚Äî‚ÄäOpticalFlow, DenseFlow, Camshift, MeanShift and Single Object Tracking with OpenCV. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Object-Tracking-with-OpenCV-and-Python-2bf91e9f6f49405ca40409c392a2d429">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/68301d6ed9614323b03031c7238b6e4c" class="NumberedList" value="4"><span class="SemanticStringArray"><span class="SemanticString">Social Distancing Violation Detection</span></span></li><li id="https://www.notion.so/ec2864f048b0435ca8770b4251b68a50" class="NumberedList" value="5"><span class="SemanticStringArray"><span class="SemanticString">People and Vehicle Counter Detection</span></span></li></ol><div id="https://www.notion.so/1cd95afaf5e74d08b6fb96f00f373831" class="Divider"></div><h1 id="https://www.notion.so/c0e8196df9cd4992ace6d41e069fada5" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/c0e8196df9cd4992ace6d41e069fada5"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Video Action Recognition</span></span></h1><div id="https://www.notion.so/5871f3c58e54403c91276cf7156156d5" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb0dcbe5d-bdfb-4229-bd27-d3bccb70e0ae%2Fimg.png?table=block&amp;id=5871f3c5-8e54-403c-9127-6cf7156156d5"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb0dcbe5d-bdfb-4229-bd27-d3bccb70e0ae%2Fimg.png?table=block&amp;id=5871f3c5-8e54-403c-9127-6cf7156156d5" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/0df3e74e04b24987a3deb0e8948abef1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">This is the task of identifying human activities/actions (e.g. eating, playing) in videos. In other words, this task classifies segments of videos into a set of pre-defined categories.</em></span></span></p></div><div id="https://www.notion.so/bc52220b4919453aa66cbafdc1aa3d19" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Applications </strong></span><span class="SemanticString">- Automated surveillance, elderly behavior monitoring, human-computer interaction, content-based video retrieval, and video summarization.</span></span></p></div><div id="https://www.notion.so/fe8b9e45133a49f2840a9bea189736b0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Scope </strong></span><span class="SemanticString">- Human Action only</span></span></p></div><div id="https://www.notion.so/85b0f089a68a4c5bb4e1f0b899505b5b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Tools </strong></span><span class="SemanticString">- OpenCV</span></span></p></div><h2 id="https://www.notion.so/fc2f18b4ab7b4842b4c900f04f9db2d2" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/fc2f18b4ab7b4842b4c900f04f9db2d2"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Models</strong></span></span></h2><h3 id="https://www.notion.so/b0a9157fc349474a90c19f639040cd03" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/b0a9157fc349474a90c19f639040cd03"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3D-ResNet</span></span></h3><div id="https://www.notion.so/28eecca7befb4f4088421c74375fb4f2" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1711.09577"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</strong></em></a></span></span></p></div><div id="https://www.notion.so/9a7119629bec4844801820d5e2be6e9b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">the authors explore how existing state-of-the-art 2D architectures (such as ResNet, ResNeXt, DenseNet, etc.) can be extended to video classification via 3D kernels.</span></span></p></div><h3 id="https://www.notion.so/fe8be93faed84b32b553e3cf435faa93" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/fe8be93faed84b32b553e3cf435faa93"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">R(2+1)D</span></span></h3><div id="https://www.notion.so/4fa16f690a414683ad80c07a8f3cb30a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">This model was pre-trained on 65 million social media videos and fine-tuned on Kinetics400.</span></span></p></div><h2 id="https://www.notion.so/dc6a275a34b64d6483f7e1c3a7cc25b1" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/dc6a275a34b64d6483f7e1c3a7cc25b1"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Use Cases</strong></span></span></h2><ol class="NumberedListWrapper"><li id="https://www.notion.so/63fadf9f582b462489ee73fec4eaa0a5" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">Kinetics 3D CNN Human Activity Recognition - This dataset consists of 400 human activity recognition classes, at least 400 video clips per class (downloaded via YouTube) and a total of 300,000 videos. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Kinetics-3D-CNN-Human-Activity-Recognition-fd10fd7b5858459cba65dc4a6cb73630">this</a></span><span class="SemanticString"> notion.</span></span></li><li id="https://www.notion.so/961ff1ee65e94c4b8522147f3dcee583" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">Action Recognition using R(2+1)D Model - VGA Annotator was used for creating the video annotation for training. Check out </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/Action-Recognition-using-R-2-1-D-Model-4c796f308aed40f29fc230a757af98e8">this</a></span><span class="SemanticString"> notion.</span></span></li></ol><div id="https://www.notion.so/7ee5dd8b8da04d76a5cb16a64711aedd" class="Divider"></div><div id="https://www.notion.so/eea1809255a14a5ebbc072aa7a510b63" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Next part is coming soon...stay tuned.</em></span></span></p></div></article>  <footer class="Footer">
        <div>&copy; Sparsh Agarwal 2021</div>
        <div>&centerdot;</div>
        <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
            rel="noopener noreferrer">Notablog</a>.
        </div>
    </footer>
</body>

</html>